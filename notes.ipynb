{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "- Web scraping is the process of extracting data from websites by simulating human browsing behavior. \n",
    " - It involves sending requests to web pages, retrieving the HTML content, and then parsing and extracting relevant information from that content. \n",
    " - The extracted data is usually stored in a structured format like CSV, Excel, or a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of Web Scraping\n",
    "\n",
    "1. HTTP Requests: These are sent to web pages to retrieve data. Commonly used methods are GET (to retrieve data) and POST (to send data).\n",
    "\n",
    "2. Web Page Parsing: Once the data is fetched, it's parsed (analyzed and interpreted) to extract the required information. This typically involves parsing the HTML or XML content.\n",
    "\n",
    "3. HTML or XML Parsing Libraries: Libraries such as BeautifulSoup, lxml, or Selenium (for dynamic content) help in parsing and navigating the page structure.\n",
    "\n",
    "4. Web Scraping Frameworks: Tools like Scrapy, Puppeteer, and Selenium help automate the scraping process, especially when the data is spread across multiple pages or involves complex interactions.\n",
    "\n",
    "5. Data Storage: After extracting the data, it needs to be stored for later use. This can be in formats like CSV, JSON, databases (SQL or NoSQL), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Elements of Web Scraping\n",
    "\n",
    "1. Target Web Page: The website or web page from which data will be scraped.\n",
    "\n",
    "2. HTML Structure: Understanding the DOM (Document Object Model) of the page is essential. This structure dictates how data can be located, extracted, and formatted.\n",
    "\n",
    "3. Selectors: CSS selectors, XPath, or regular expressions are used to identify the specific data points in the HTML content.\n",
    "\n",
    "4. Requests/Headers: Requests often need to include headers, such as user-agent strings, to mimic real user behavior and avoid being blocked by the server.\n",
    "\n",
    "5. Response Handling: After sending a request, handling the response (successful or failed) is important for the scraping process.\n",
    "\n",
    "6. Rate Limiting & Throttling: To avoid being detected or blocked by the website, scraping should be done at a reasonable rate, often with delays or random intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases of Web Scraping\n",
    "\n",
    "1. Price Comparison: Businesses and individuals use web scraping to collect pricing information from competitors' websites and compare prices in real-time.\n",
    "\n",
    "2. Data Mining: Scraping can be used for research purposes, such as gathering data from online publications, blogs, and social media for analysis.\n",
    "\n",
    "3. News Aggregation: News websites aggregate headlines, summaries, and articles from various news sources using scraping.\n",
    "\n",
    "4. Job Listings: Websites like Indeed or LinkedIn can be scraped to collect job postings and aggregate them into a custom platform.\n",
    "\n",
    "5. Market Research: Collecting reviews, ratings, and product information from e-commerce sites for analysis or business insights.\n",
    "\n",
    "6. Real Estate Listings: Extracting property listings and details from real estate websites to build comparison tools or databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler vs Scraper\n",
    "\n",
    "- Crawlers explore the web by following links to discover and index pages, often used by search engines.\n",
    "- Scrapers extract specific data (like text or images) from web pages, often used for gathering targeted information.\n",
    "    \n",
    "            Crawlers are used to find data, while scrapers are used to collect it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/crawler-scraper.png\" width=\"1000\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping vs Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Web Scraping is about extracting raw data from web pages, while Web Wrangling is about cleaning, transforming, and organizing that data to make it usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping Libraries (Python)\n",
    "\n",
    "- BeautifulSoup and Requests are great for simple static web scraping.\n",
    "- Scrapy is a powerful framework for large-scale scraping projects.\n",
    "- Selenium and Pyppeteer are ideal for scraping dynamic content (JavaScript-heavy websites)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping Life Cycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./assets/what-is-web-scraping.jpg' width='1000' height='650'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules and Regulations for ethical web scraping in:\n",
    "\n",
    "1. Respect Terms of Service: Always review and follow the website's Terms of Service and robots.txt file.\n",
    "2. Avoid Overloading Servers: Scrape at a reasonable rate and avoid excessive requests that could overwhelm the website.\n",
    "3. Use Transparent Methods: Identify your scraper clearly and avoid hidden scraping activities.\n",
    "4. Don’t Scrape Sensitive Data: Avoid scraping personal, confidential, or sensitive information without permission.\n",
    "5. Respect Copyright: Don’t scrape copyrighted content unless you have permission.\n",
    "6. Use Data Responsibly: Ensure data is used for its intended purpose and not for unethical activities.\n",
    "7. Give Credit to Sources: Attribute the original source when using scraped data publicly.\n",
    "8. Monitor Impact: Be mindful of how your scraping affects website performance and avoid negative impacts.\n",
    "\n",
    "                   prefered API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Module\n",
    "\n",
    "- pip install requests\n",
    "- pip install beautifulsoup4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
